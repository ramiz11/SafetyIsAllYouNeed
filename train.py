"""
Train a causal LM (LoRA, 4-bit) on the textual trajectory prompts
generated by the preprocessing pipeline.

Notes
-----
- Make sure you `pip install` the following in your environment:
    transformers accelerate peft datasets bitsandbytes huggingface_hub
- Login to Hugging Face if the model is gated.
- This script does not use argparse; call run_train(...) directly.
"""

import os
import json
import shutil
from typing import Iterable, Tuple
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from configs import preprocessing_config as pc


def _resolve_prompt_paths(use_safety: bool) -> Tuple[str, str, str]:
    """
    Return (train_json, val_json, test_json) absolute paths for prompt files,
    picking from CURRENT_DATA_DIR or SAFETY_DATA_DIR as appropriate.
    """
    if use_safety:
        data_dir = pc.SAFETY_DATA_DIR
        train_json = os.path.join(data_dir, "safety_textual_train_trajs.json")
        val_json = os.path.join(data_dir, "safety_textual_validation_trajs.json")
        test_json = os.path.join(data_dir, "safety_textual_test_trajs.json")
    else:
        data_dir = pc.CURRENT_DATA_DIR
        train_json = os.path.join(data_dir, "textual_train_trajs.json")
        val_json = os.path.join(data_dir, "textual_validation_trajs.json")
        test_json = os.path.join(data_dir, "textual_test_trajs.json")

    for p in (train_json, val_json, test_json):
        if not os.path.exists(p):
            raise FileNotFoundError(f"Prompt file not found: {p}")
    return train_json, val_json, test_json


def _load_hf_datasets(train_json: str, val_json: str, test_json: str):
    with open(train_json, "r") as f:
        train_texts = json.load(f)
    with open(val_json, "r") as f:
        val_texts = json.load(f)
    with open(test_json, "r") as f:
        test_texts = json.load(f)

    train_ds = Dataset.from_dict({"text": train_texts})
    val_ds = Dataset.from_dict({"text": val_texts})
    test_ds = Dataset.from_dict({"text": test_texts})
    return train_ds, val_ds, test_ds


def make_tokenize_and_mask_fn(tokenizer, max_length: int):
    """
    Returns a function that:
      - tokenizes text
      - creates labels identical to input_ids
      - masks all tokens up to the start of "<answer>:" with -100 (ignored in loss)
    """
    answer_marker = "<answer>:"

    def _fn(batch):
        # Encode full text
        enc = tokenizer(
            batch["text"],
            padding="longest",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
        input_ids = enc["input_ids"]
        labels = input_ids.clone()

        # For each sample, figure out how many tokens belong to the question part
        for i, full_text in enumerate(batch["text"]):
            idx = full_text.find(answer_marker)
            if idx == -1:
                # No answer marker; ignore the whole sample in loss
                labels[i, :] = -100
                continue

            # Tokenize the substring up to answer start; its token length is the mask cutoff
            q_text = full_text[:idx]
            q_enc = tokenizer(
                q_text,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
            )
            q_len = q_enc["input_ids"].shape[1]

            # If the full sequence got truncated before reaching the answer, we may end up
            # masking the entire thing (OKâ€”no loss from this sample).
            cutoff = min(q_len, labels.size(1))
            labels[i, :cutoff] = -100

        enc["labels"] = labels
        return enc

    return _fn


def run_train(
    *,
    dataset: str = "NYC", # or "CHICAGO"
    traj_len: int = 10, # or any other length
    crime_radius: int = 1000, # or any other length
    crime_time_weeks: int = 4, # or any other length
    base_dir: str = "/Users/ramizaboura/MSC/SafetyIsAllYouNeed", # change to your own dir
    use_safety: bool = True, # or False
    # Model / tokenizer
    model_name: str = "meta-llama/Llama-3.1-8B-Instruct",
    use_4bit: bool = True,
    torch_dtype = torch.float16,
    # LoRA fine tuning config
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
    target_modules: Iterable[str] = ("q_proj", "v_proj"),
    # Tokenization
    max_length: int = 2048,
    # Training
    num_train_epochs: int = 3,
    lr: float = 2e-5,
    warmup_steps: int = 20,
    weight_decay: float = 0.0,
    per_device_train_batch_size: int = 1,
    gradient_accumulation_steps: int = 1,
    save_eval_steps: int = 500,              # save & eval interval
    fp16: bool = True,
    # Output
    output_root: str = "models",             # relative to base_dir
):
    """
    Single training run with the given hyperparameters.
    """
    # Config paths for this dataset/hparam combo
    pc.update_config(dataset, traj_len, crime_radius, crime_time_weeks, base_dir=base_dir)
    # Locate training files
    train_json, val_json, test_json = _resolve_prompt_paths(use_safety)
    train_ds, val_ds, test_ds = _load_hf_datasets(train_json, val_json, test_json)
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # Model (4-bit quantized by default) + LoRA fine-tuning
    quant_cfg = None
    if use_4bit:
        quant_cfg = BitsAndBytesConfig(load_in_4bit=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch_dtype,
        quantization_config=quant_cfg,
    )

    lora_cfg = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=list(target_modules),
        lora_dropout=lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()
    # Tokenization + masking
    tok_fn = make_tokenize_and_mask_fn(tokenizer, max_length=max_length)
    train_tok = train_ds.map(tok_fn, batched=True, remove_columns=["text"])
    val_tok = val_ds.map(tok_fn, batched=True, remove_columns=["text"])
    test_tok = test_ds.map(tok_fn, batched=True, remove_columns=["text"])  # optional

    train_tok.set_format(type="torch")
    val_tok.set_format(type="torch")
    test_tok.set_format(type="torch") # optional
    # Training args
    # Build a scoped output dir, e.g. models/CHICAGO/traj_len-15/crime_radius-250m/crime_time-4w/Llama-3.1-8B-Instruct_w_safety
    scope_dir = os.path.join(
        base_dir,
        output_root,
        pc.DATASET,
        f"traj_len-{traj_len}",
        f"crime_radius-{crime_radius}m",
        f"crime_time-{crime_time_weeks}w",
        f"{model_name.split('/')[-1]}_{'w' if use_safety else 'wo'}_safety",
    )
    os.makedirs(scope_dir, exist_ok=True)
    training_args = TrainingArguments(
        output_dir=scope_dir,
        num_train_epochs=num_train_epochs,
        learning_rate=lr,
        warmup_steps=warmup_steps,
        weight_decay=weight_decay,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        evaluation_strategy="steps",
        save_strategy="steps",
        eval_steps=save_eval_steps,
        save_steps=save_eval_steps,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
        logging_steps=save_eval_steps,
        save_total_limit=3,
        fp16=fp16,
        dataloader_num_workers=2,
        report_to="none",  # set to "wandb" or "tensorboard" if desired
    )
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=val_tok,
        tokenizer=tokenizer,
    )
    # Train
    torch.cuda.empty_cache()
    trainer.train()
    # Persist the best checkpoint to a clean "best" folder + tokenizer
    best_ckpt = trainer.state.best_model_checkpoint or scope_dir
    final_dir = os.path.join(scope_dir, "best")
    os.makedirs(final_dir, exist_ok=True)
    shutil.copytree(best_ckpt, final_dir, dirs_exist_ok=True)
    tokenizer.save_pretrained(final_dir)
    print(f"Best checkpoint: {best_ckpt}")
    print(f"Final model (with tokenizer) saved to: {final_dir}")


if __name__ == "__main__":
    ## Configure (change config of) dataset, base directory, and trajectory related hyperparameters here.
    ## For reproducibility, keep the remaining training settings unchanged to match our best results.
    run_train(
        dataset="NYC",
        traj_len=10,
        crime_radius=1000,
        crime_time_weeks=4,
        base_dir="/Users/ramizaboura/MSC/SafetyIsAllYouNeed",
        use_safety=True,
        model_name="meta-llama/Llama-3.1-8B-Instruct",
        max_length=2048,
        num_train_epochs=3,
        lr=2e-5,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        save_eval_steps=500,
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=("q_proj","v_proj"),
    )
