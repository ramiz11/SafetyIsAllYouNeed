MODELS_DIR = "../models"
BASE_MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct" # this is the model that will be loaded as the pretrained backbone
TO_SAVE_MODEL_NAME = 'Llama-3.1-8B-Instruct'
LOAD_IN_4_BITS_FLAG = True
LORA_ALPHA = 32
LORA_RANK = 16
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = ["q_proj", "v_proj"]
LORA_BIAS = 'none'
TOKENIZATION_PADDING_MODE = "longest"
TOKENIZATION_TRUNCATION_FLAG = True
MAX_TOKEN_LENGTH = 4096
MAX_NEW_TOKENS_INFERENCE = 128
USE_SAFETY = False # Set to true when you want to train on trajectories with integrated safety.
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
WARMUP_STEPS = 20
WEIGHT_DECAY = 0.0
BATCH_SIZE_PER_GPU = 1
GRADIENT_ACCUMULATION_STEPS = 1
EVAL_STRATEGY = "epoch"
LOGGING_STEPS = 50
SAVE_STEPS = 500
SAVE_TOTAL_LIMIT = 3
BIT_PRECISION_16_FLAG = True
